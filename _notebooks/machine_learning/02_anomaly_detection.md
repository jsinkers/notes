---
title: Anomaly Detection
notebook: Machine Learning
layout: note
date: 2021-06-15
tags: 
...

## What are outliers/anomalies?

- __anomaly:__ data object that __deviates significantly__ from normal objects as if it were __generated by a different mechanism__
  - e.g. unusual credit card purchase
  -  distinct from noise:
    - noise is random error/variance in measured variable
    - should be removed prior to anomaly detection
  - are __interesting__ as violations of mechanism generating normal data
    - translate to significant/critical real life entities
    - e.g. cyber intrusion, credit card fraud
- example of CCTV camera performing facial recognition
  - anomaly: new face encountered
  - noise: variation in lighting; person is wearing a mask

## Variants of Anomaly detection problems

- given database $D$ find all data points $x\in D$ with anomaly scores greater than __threshold $t$__
- given database $D$ find $n$ data points $x\in D$ with __highest anomaly scores__
- given database $D$ of mostly normal, unlabelled data, and a test point $x$, compute its __anomaly score__ with respect to $D$
  - how well is some data point explained

## Types of Anomalies

### Global/Point anomaly

- object significantly deviates from rest of data set
- e.g. intrusion detection in computer networks
- issue: appropriate measure of deviation

![global anomaly](img/global-anomaly.png)

### Contextual/Conditional Anomaly

- object deviates significantly based on selected context
- attributes need to be classified according to 
  - contextual attributes: define context e.g. time, location
  - behavioural attributes: object characteristics used for anomaly evaluation, e.g. temperature
- generalisation of local anomalies whose density significantly deviates from local area
- issue: defining meaningful context
- example: 10Â°C in Paris: is this an anomaly?
  - in June: yes
  - in December: no

### Collective Anomaly

- subset of objects collectively deviate significantly from the whole data set, even if individual data objects may not be anomalies
- e.g. intrusion detection
  - mistype password once, producing a DoS packet: not anomalous
  - collection of DOS packets all at once: anomalous
- detection
  - consider behaviour of groups of objects
  - requires background knowledge of relationship among data objects, e.g. distance or similarity measure
- requires a relationship among data instances, whether
  - sequential data
  - spatial data
  - graph data
- individual instances are not anomalous by themselves
- e.g. ECG of normal heart rate with sudden flatline: you need background knowledge to know what normal ECG looks like

## Anomaly Detection Paradigms

### Supervised

- labels available for both normal data and anomalies: unrealistic to expect in reality
- samples that have been examined by domain expert are used for train and test
  - e.g. medical domain and ECG
- challenge
  - obtaining labels for both normal and anomalous data
  - imbalanced classes: anomalies are rare
    - could boost anomaly class and make up artificial anomalies
  - cannot detect unknown/emerging anomalies
  - catch as many outliers as possible, i.e. __recall__ more important than accuracy.  We don't want to mislabel normal objects as outliers

### Semi-Supervised

- labels only available for normal data, a more typical scenario
- model normal objects, then report those not matching the model as outliers
- challenges
  - requires labels from normal class
  - may get high false alarm rate from unseen legitimate records

### Unsupervised Anomaly Detection

- no labels available
- assume normal objects are clustered into multiple groups having distinct features
- outlier is expected to be far away from any groups of normal objects
- steps
  - build profile of normal behaviour through:
    - summary statistics for overall population
    - model of multivariate data distribution
  - use normal profile to detect anomaly, as points varying significantly from normal profile
- challenges
  - no guarantee normal objects will share strong patterns
  - possible outliers may share high similarity in a small area
  - e.g. in intrusion/virus detection, normal activities are diverse: unsupervised methods may have high FP rate and miss real outliers
- many clustering methods can be used for anomaly detection
  - find clusters, then outliers are those points not belonging to any cluster
  - problem 1: distinguishing noise from outliers
  - problem 2: costly since first clustering; far less outliers than normal objects

## Unsupervised Anomaly Detection Approaches

- statistical: assume normal data follow some statistical model
- proximity-based: object is an outlier if the nearest neighbours of the object are far away
- density-based: outliers are objects in regions of low density
- clustering-based: normal data belong to large, dense clusters

## Statistical anomaly detection

- anomalies are objects fit poorly by a statistical model
- idea: learn a model fitting given data set
  - identify objects in low probability regions as anomalous
- assumption: normal data is generated by parametric distribution with parameter $\theta$
  - PDF of parametric distribution, $f(x,\theta)$, gives the probability that object $x$ is generated by the distribution
  - the smaller the value, the more likely $x$ is an outlier
- challenges
  - dependent on assumption of statistical model holding for the data

### Pros

- theoretically well-founded
- statistical tests well understood, well validated
- quantitative measure of degree to which an object is an outlier

### Cons

- data may be hard to model parametrically
  - multiple modes
  - varying density
- in high dimensions, data may be insufficient to estimate true distribution

### Graphical Approaches

- boxplot (1D), scatter plot (2D), spin plot (3D)
- time consuming, subjective

### Univariate data

- assuming univariate Gaussian distribution
- use maximum likelihood method to estimate $\mu, \sigma$

$$\hat\mu = \frac{1}{n}\sum_{i=1}^{n}x_i$$
$$\hat\sigma^2 = \frac{1}{n}\sum_{i=1}^{n}(x_i-\hat\mu)^2$$

- choose confidence limits, e.g. $3\sigma$
  - $\mu\pm3\sigma$ covers 99.7% of data

### Multivariate data

- multivariate Gaussian distribution
- outliers defined by __Mahalanobis distance__
- apply __Grubb's test__ on the distances

![Mahalanobis](img/mahalanobis.png)

- in above image, Euclidean distance for B is greater than for A
- but there is much more variation on the $x$ axis, so we want to standardise such that each dimension has the same variation and range
- there is also correlation between x and y which needs to be controlled for
- Mahalanobis Distance

$$y^2 = (x-\bar x)' S^{-1} (x-\bar x)$$
- $S$: covariance matrix
$$S = \frac{1}{n-1}\sum_{i=1}^{n}(x_i)-\bar x)(x_i-\bar x)'$$

### Likelihood Approach

- assume dataset $D$ contains samples from a mixture of 2 probability distributions:
  - $M$: majority distribution, estimated from data
  - $A$: anomalous distribution, initially assumed to be uniform
- approach
  - initially assume all data points belong to $M$ - estimate majority distribution
  - let $L_t(D)$ be log-likelihood of $D$ at time $t$
  - for each point $x_t\in M$ move it to $A$ - i.e. test whether it is an anomaly
    - compute the difference $\Delta = L_t(D)-L_{t+1}(D)$
    - if $\Delta > c$, some threshold value, then $x_t$ declared an anomaly and permanently moved to $A$


